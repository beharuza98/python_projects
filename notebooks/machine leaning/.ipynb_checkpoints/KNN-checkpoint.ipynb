{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.605551275463989\n",
      "7.810249675906654\n"
     ]
    }
   ],
   "source": [
    "def euclidean_distance(pt1,pt2):\n",
    "  distance=0\n",
    "  for i in range(len(pt1)):\n",
    "    distance +=(pt1[i]-pt2[i])**2\n",
    "  return distance**0.5\n",
    "print(euclidean_distance([1, 2],[4, 0]))\n",
    "print(euclidean_distance([5, 4, 3],[1, 7, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manhattan distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "def manhattan_distance(pt1,pt2):\n",
    "  distance=0\n",
    "  for i in range(len(pt1)):\n",
    "    distance += abs(pt1[i]-pt2[i])\n",
    "  return distance\n",
    "\n",
    "print(manhattan_distance([1, 2], [4, 0]))\n",
    "print(manhattan_distance([5, 4, 3], [1, 7, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hamming distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "def hamming_distance(pt1,pt2):\n",
    "  distance=0\n",
    "  for i in range(len(pt1)):\n",
    "    if pt1[i] != pt2[i]:\n",
    "      distance+=1\n",
    "  return distance\n",
    "print(hamming_distance([1, 2],[1, 100]))\n",
    "print(hamming_distance([5, 4, 9], [1, 7, 9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclidean Distance .euclidean()\n",
    "Manhattan Distance .cityblock()\n",
    "Hamming Distance .hamming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.605551275463989\n",
      "5\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print(distance.euclidean([1, 2],[4, 0]))\n",
    "print(distance.cityblock([1, 2],[4, 0]))\n",
    "print(distance.hamming([5, 4, 9],[1, 7, 9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance between points 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.770329614269007\n",
      "38.897300677553446\n"
     ]
    }
   ],
   "source": [
    "star_wars = [125, 1977]\n",
    "raiders = [115, 1981]\n",
    "mean_girls = [97, 2004]\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  length_difference = (movie1[0] - movie2[0]) ** 2\n",
    "  year_difference = (movie1[1] - movie2[1]) ** 2\n",
    "  distance = (length_difference + year_difference) ** 0.5\n",
    "  return distance\n",
    "\n",
    "print(distance(star_wars, raiders))\n",
    "print(distance(star_wars, mean_girls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Between Points - 3D\n",
    "Making a movie rating predictor based on just the length and release date of movies is pretty limited. There are so many more interesting pieces of data about movies that we could use! So let’s add another dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000000.000008286\n",
      "6000000.000126083\n"
     ]
    }
   ],
   "source": [
    "star_wars = [125, 1977, 11000000]\n",
    "raiders = [115, 1981, 18000000]\n",
    "mean_girls = [97, 2004, 17000000]\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "print(distance(star_wars, raiders))\n",
    "print(distance(star_wars, mean_girls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data with Different Scales: Normalization\n",
    "In the next three lessons, we’ll implement the three steps of the K-Nearest Neighbor Algorithm:\n",
    "\n",
    "Normalize the data\n",
    "Find the k nearest neighbors\n",
    "Classify the new point based on those neighbors\n",
    "When we added the dimension of budget, you might have realized there are some problems with the way our data currently looks.\n",
    "\n",
    "Consider the two dimensions of release date and budget. The maximum difference between two movies’ release dates is about 125 years (The Lumière Brothers were making movies in the 1890s). However, the difference between two movies’ budget can be millions of dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.047619047619047616, 0.8492063492063492, 0.8650793650793651, 0.4523809523809524, 0.5634920634920635, 0.46825396825396826, 0.6666666666666666, 0.5476190476190477, 1.0, 0.36507936507936506, 0.6111111111111112, 0.8333333333333334, 0.42063492063492064, 0.0, 0.8253968253968254, 0.4523809523809524, 0.9523809523809523, 0.5873015873015873, 0.0, 0.6904761904761905]\n",
      "0.047619047619047616\n"
     ]
    }
   ],
   "source": [
    "release_dates = [1897, 1998, 2000, 1948, 1962, 1950, 1975, 1960, 2017, 1937, 1968, 1996, 1944, 1891, 1995, 1948, 2011, 1965, 1891, 1978]\n",
    "def min_max_normalize(lst):\n",
    "  minimum =min(lst)\n",
    "  maximum=max(lst)\n",
    "  normalized=[]\n",
    "  for value in lst:\n",
    "    normalized_num = (value - minimum) / (maximum - minimum)\n",
    "    normalized.append(normalized_num)\n",
    "  return normalized \n",
    "print(min_max_normalize(release_dates))\n",
    "print(min_max_normalize(release_dates)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Nearest Neighbors\n",
    "The K-Nearest Neighbor Algorithm:\n",
    "\n",
    "Normalize the data\n",
    "Find the k nearest neighbors\n",
    "Classify the new point based on those neighbors\n",
    "Now that our data has been normalized and we know how to find the distance between two points, we can begin classifying unknown data!\n",
    "\n",
    "To do this, we want to find the k nearest neighbors of the unclassified point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movies import movie_dataset, movie_labels\n",
    "\n",
    "#print(movie_dataset['Bruce Almighty'])\n",
    "#print(movie_labels['Bruce Almighty'])\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  return neighbors\n",
    "  \n",
    "print(classify([.4, .2, .9], movie_dataset, 5))\n",
    "\n",
    "#[[0.08273614694606074, 'Lady Vengeance'], [0.22989623153818367, 'Steamboy'], [0.23641372358159884, 'Fateless'], [0.26735445689589943, 'Princess Mononoke'], [0.3311022951533416, 'Godzilla 2000']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Neighbors\n",
    "The K-Nearest Neighbor Algorithm:\n",
    "\n",
    "Normalize the data\n",
    "Find the k nearest neighbors\n",
    "Classify the new point based on those neighbors\n",
    "\n",
    "Our goal now is to count the number of good movies and bad movies in the list of neighbors. If more of the neighbors were good, then the algorithm will classify the unknown movie as good. Otherwise, it will classify it as bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movies import movie_dataset, movie_labels\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, labels, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for neighbor in neighbors:\n",
    "    title = neighbor[1]\n",
    "    if labels[title] == 0:\n",
    "      num_bad += 1\n",
    "    elif labels[title] == 1:\n",
    "      num_good += 1\n",
    "  if num_good > num_bad:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "print(classify([.4, .2, .9], movie_dataset, movie_labels, 5))\n",
    "\n",
    "#result= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Your Favorite Movie\n",
    "Your classifier is now able to predict whether a movie will be good or bad. So far, we’ve only tested this on a completely random point [.4, .2, .9]. In this exercise we’re going to pick a real movie, normalize it, and run it through our classifier to see what it predicts!\n",
    "we are going to be testing our classifier using the 2017 movie Call Me By Your Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movies import movie_dataset, movie_labels, normalize_point\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, labels, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for neighbor in neighbors:\n",
    "    title = neighbor[1]\n",
    "    if labels[title] == 0:\n",
    "      num_bad += 1\n",
    "    elif labels[title] == 1:\n",
    "      num_good += 1\n",
    "  if num_good > num_bad:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "  \n",
    "print(\"Call Me By Your Name\" in movie_dataset)\n",
    "my_movie = [3500000, 132, 2017]\n",
    "normalized_my_movie = normalize_point(my_movie)\n",
    "print(normalized_my_movie)\n",
    "print(classify(normalized_my_movie, movie_dataset, movie_labels, 5))\n",
    "\n",
    "#False\n",
    "#[0.00028650338197026213, 0.3242320819112628, 1.0112359550561798]\n",
    "#1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Sets\n",
    "You’ve now built your first K Nearest Neighbors algorithm capable of classification. You can feed your program a never-before-seen movie and it can predict whether its IMDb rating was above or below 7.0. However, we’re not done yet. We now need to report how effective our algorithm is. After all, it’s possible our predictions are totally wrong!\n",
    "\n",
    "As with most machine learning algorithms, we have split our data into a training set and validation set.\n",
    "\n",
    "Once these sets are created, we will want to use every point in the validation set as input to the K Nearest Neighbor algorithm. We will take a movie from the validation set, compare it to all the movies in the training set, find the K Nearest Neighbors, and make a prediction. After making that prediction, we can then peek at the real answer (found in the validation labels) to see if our classifier got the answer correct.\n",
    "\n",
    "If we do this for every movie in the validation set, we can count the number of times the classifier got the answer right and the number of times it got it wrong. Using those two numbers, we can compute the validation accuracy.\n",
    "\n",
    "Validation accuracy will change depending on what K we use. In the next exercise, we’ll use the validation accuracy to pick the best possible K for our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movies import training_set, training_labels, validation_set, validation_labels\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, labels, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for neighbor in neighbors:\n",
    "    title = neighbor[1]\n",
    "    if labels[title] == 0:\n",
    "      num_bad += 1\n",
    "    elif labels[title] == 1:\n",
    "      num_good += 1\n",
    "  if num_good > num_bad:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "  \n",
    "print(validation_set[\"Bee Movie\"])\n",
    "print(validation_labels[\"Bee Movie\"])\n",
    "\n",
    "guess=classify(validation_set[\"Bee Movie\"], training_set, training_labels, 5)\n",
    "print(guess)\n",
    "if guess == validation_labels[\"Bee Movie\"]:\n",
    "  print(\"Correct!\")\n",
    "else:\n",
    "  print(\"Wrong!\")\n",
    "\n",
    "\n",
    "#[0.012279463360232739, 0.18430034129692832, 0.898876404494382]\n",
    "0\n",
    "0\n",
    "Correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing K\n",
    "In the previous exercise, we found that our classifier got one point in the training set correct. Now we can test every point to calculate the validation accuracy.\n",
    "\n",
    "The validation accuracy changes as k changes. The first situation that will be useful to consider is when k is very small. Let’s say k = 1. We would expect the validation accuracy to be fairly low due to overfitting. Overfitting is a concept that will appear almost any time you are writing a machine learning algorithm. Overfitting occurs when you rely too heavily on your training data; you assume that data in the real world will always behave exactly like your training data. In the case of K-Nearest Neighbors, overfitting happens when you don’t consider enough neighbors. A single outlier could drastically determine the label of an unknown point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom movies import training_set, training_labels, validation_set, validation_labels\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "  squared_difference = 0\n",
    "  for i in range(len(movie1)):\n",
    "    squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "  final_distance = squared_difference ** 0.5\n",
    "  return final_distance\n",
    "\n",
    "def classify(unknown, dataset, labels, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for neighbor in neighbors:\n",
    "    title = neighbor[1]\n",
    "    if labels[title] == 0:\n",
    "      num_bad += 1\n",
    "    elif labels[title] == 1:\n",
    "      num_good += 1\n",
    "  if num_good > num_bad:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "  \n",
    "def find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, k):\n",
    "  num_correct = 0.0\n",
    "  for title in validation_set:\n",
    "    guess = classify(validation_set[title], training_set, training_labels, k)\n",
    "    if guess == validation_labels[title]:\n",
    "      num_correct += 1\n",
    "  return num_correct / len(validation_set)\n",
    "\n",
    "\n",
    "print(find_validation_accuracy(training_set, training_labels, validation_set, validation_labels, 3))\n",
    "\n",
    "#0.6639344262295082"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn\n",
    "You’ve now written your own K-Nearest Neighbor classifier from scratch! However, rather than writing your own classifier every time, you can use Python’s sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom movies import movie_dataset, labels\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier=KNeighborsClassifier(n_neighbors = 5)\n",
    "classifier.fit(movie_dataset,labels)\n",
    "print(classifier.predict([[.45, .2, .5], [.25, .8, .9],[.1, .1, .9]])) #those three numbers associated with a movie are the normalized budget, run time, and year of release.\n",
    "\n",
    "#[1 1 0] good good bad movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
